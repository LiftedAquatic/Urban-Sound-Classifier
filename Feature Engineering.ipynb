{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import librosa as lr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "\n",
    "audio_df = pd.read_csv('/Users/spence/Documents/GitHub/Urban Sounds Classifier/UrbanSound8K/metadata/audio_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>slice_file_name</th>\n      <th>fsID</th>\n      <th>start</th>\n      <th>end</th>\n      <th>salience</th>\n      <th>fold</th>\n      <th>classID</th>\n      <th>class</th>\n      <th>num_channels</th>\n      <th>sample_rate</th>\n      <th>bit_depth</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100032-3-0-0.wav</td>\n      <td>100032</td>\n      <td>0.000000</td>\n      <td>0.317551</td>\n      <td>1</td>\n      <td>5</td>\n      <td>3</td>\n      <td>dog_bark</td>\n      <td>2</td>\n      <td>44100</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100263-2-0-117.wav</td>\n      <td>100263</td>\n      <td>58.500000</td>\n      <td>62.500000</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>children_playing</td>\n      <td>2</td>\n      <td>44100</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100263-2-0-121.wav</td>\n      <td>100263</td>\n      <td>60.500000</td>\n      <td>64.500000</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>children_playing</td>\n      <td>2</td>\n      <td>44100</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100263-2-0-126.wav</td>\n      <td>100263</td>\n      <td>63.000000</td>\n      <td>67.000000</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>children_playing</td>\n      <td>2</td>\n      <td>44100</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100263-2-0-137.wav</td>\n      <td>100263</td>\n      <td>68.500000</td>\n      <td>72.500000</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>children_playing</td>\n      <td>2</td>\n      <td>44100</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8719</th>\n      <td>99812-1-2-0.wav</td>\n      <td>99812</td>\n      <td>159.522205</td>\n      <td>163.522205</td>\n      <td>2</td>\n      <td>7</td>\n      <td>1</td>\n      <td>car_horn</td>\n      <td>2</td>\n      <td>44100</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>8720</th>\n      <td>99812-1-3-0.wav</td>\n      <td>99812</td>\n      <td>181.142431</td>\n      <td>183.284976</td>\n      <td>2</td>\n      <td>7</td>\n      <td>1</td>\n      <td>car_horn</td>\n      <td>2</td>\n      <td>44100</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>8721</th>\n      <td>99812-1-4-0.wav</td>\n      <td>99812</td>\n      <td>242.691902</td>\n      <td>246.197885</td>\n      <td>2</td>\n      <td>7</td>\n      <td>1</td>\n      <td>car_horn</td>\n      <td>2</td>\n      <td>44100</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>8722</th>\n      <td>99812-1-5-0.wav</td>\n      <td>99812</td>\n      <td>253.209850</td>\n      <td>255.741948</td>\n      <td>2</td>\n      <td>7</td>\n      <td>1</td>\n      <td>car_horn</td>\n      <td>2</td>\n      <td>44100</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>8723</th>\n      <td>99812-1-6-0.wav</td>\n      <td>99812</td>\n      <td>332.289233</td>\n      <td>334.821332</td>\n      <td>2</td>\n      <td>7</td>\n      <td>1</td>\n      <td>car_horn</td>\n      <td>2</td>\n      <td>44100</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n<p>8724 rows Ã— 11 columns</p>\n</div>",
      "text/plain": "         slice_file_name    fsID       start         end  salience  fold  \\\n0       100032-3-0-0.wav  100032    0.000000    0.317551         1     5   \n1     100263-2-0-117.wav  100263   58.500000   62.500000         1     5   \n2     100263-2-0-121.wav  100263   60.500000   64.500000         1     5   \n3     100263-2-0-126.wav  100263   63.000000   67.000000         1     5   \n4     100263-2-0-137.wav  100263   68.500000   72.500000         1     5   \n...                  ...     ...         ...         ...       ...   ...   \n8719     99812-1-2-0.wav   99812  159.522205  163.522205         2     7   \n8720     99812-1-3-0.wav   99812  181.142431  183.284976         2     7   \n8721     99812-1-4-0.wav   99812  242.691902  246.197885         2     7   \n8722     99812-1-5-0.wav   99812  253.209850  255.741948         2     7   \n8723     99812-1-6-0.wav   99812  332.289233  334.821332         2     7   \n\n      classID             class  num_channels  sample_rate  bit_depth  \n0           3          dog_bark             2        44100         16  \n1           2  children_playing             2        44100         16  \n2           2  children_playing             2        44100         16  \n3           2  children_playing             2        44100         16  \n4           2  children_playing             2        44100         16  \n...       ...               ...           ...          ...        ...  \n8719        1          car_horn             2        44100         16  \n8720        1          car_horn             2        44100         16  \n8721        1          car_horn             2        44100         16  \n8722        1          car_horn             2        44100         16  \n8723        1          car_horn             2        44100         16  \n\n[8724 rows x 11 columns]"
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df['sound_len'] = audio_df.end - audio_df.start "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will extract some features from the audio files. There are several audio characteristics that can be derived from our samples...\n",
    "\n",
    "1. Mel Frequency Cepstral Cooeficient - This is by far the most popular feature extraction method for audio files. It makes use of the Mel Scale, which is a perceptual scale of pitches judged by listeners to be equally spaced apart. There is a formula for converting *hertz* to *mels* that makes use of a logarithmic function.  \n",
    "\n",
    "MFCC's are dervied as follows: [Read more about MFCC's here](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)\n",
    "1. Take the Fourier transform of (a windowed excerpt of) a signal.\n",
    "2. Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows or alternatively, cosine overlapping windows.\n",
    "3. Take the logs of the powers at each of the mel frequencies.\n",
    "4. Take the discrete cosine transform of the list of mel log powers, as if it were a signal.\n",
    "5. The MFCCs are the amplitudes of the resulting spectrum.\n",
    "\n",
    "Essentially, the MFCC function creates a frequency/amplitude-based image of the audio file. We will then feed these images into a Convolutional Neural Network and classify them, just like we would with regular image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the Mel Frequency Cepstral Cooeficient for each sound \n",
    "mfccs = []\n",
    "\n",
    "# iterate through dataframe\n",
    "for index, row in audio_df.iterrows():\n",
    "    #access file path for each audio file\n",
    "    f_name= '/Users/spence/Documents/GitHub/Urban Sounds Classifier/UrbanSound8K/audio/fold' + str(row[\"fold\"]) + '/' + str(row[\"slice_file_name\"])\n",
    "    # use librosa to load the file\n",
    "    audio_ts, sr = lr.load(f_name, res_type='kaiser_fast')\n",
    "    # use librosa function to extract mfcc's\n",
    "    mel_freq = lr.feature.mfcc(y=audio_ts, sr=sr,  n_mfcc=40)\n",
    "    # set a padding around edges of sample, we use 174 because that correlates to the longest sample we have \n",
    "    padding = 174 - mel_freq.shape[1]\n",
    "    # apply padding \n",
    "    mfcc = np.pad(mel_freq, pad_width=((0, 0), (0, padding)), mode='constant')\n",
    "    # mel_scaled = np.mean(mel_freq.T, axis=0)\n",
    "    # append each mfcc to list \n",
    "    mfccs.append(mfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(8724, 40, 174)"
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the length of each item in nested list \n",
    "\n",
    "len(mfccs) , len(mfccs[0]), len(mfccs[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the dimensions of the list(mfccs) we created above:\n",
    "\n",
    "1. We have a list with 8,724 items; one item for each audio file. 8,724 is also the amount of rows in the dataframe.\n",
    "2. Within each of the aforementioned items we have an array that contains 40 lists. Each of those lists is an mfcc for that audio file(item). In the for-loop above we set n_mfcc to 40. The standard amount of mfccs to generate is 20-40. \n",
    "3. Each of those 40 mfcc's contains 174 integers. These are the numbers that essentially characterize the sound. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add extracted the mfcc's to dataframe as column \n",
    "\n",
    "audio_df['mfccs'] = mfccs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe \n",
    "\n",
    "audio_df.to_pickle('audio_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Chromagrams - The chroma feature of an audio file is known as a 'pitch class profile'. The values dervied from this extraction are typically used for music classification, due to the emphasis on classifying tone or pitch. I am interested in applying this method to urban sound because chroma features can supposedly capture changes in timbre. Timbre is the sound quality of a tone and might be useful in distinguishing between similar but distinct sounds, like two droning sounds that are generated by different sources; ex: an engine and an air conditioner. \n",
    "\n",
    "[More on Chromagrams](https://en.wikipedia.org/wiki/Chroma_feature#:~:text=Shifting%20the%20time%20window%20across,also%20referred%20to%20as%20chromagram.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the Chromagram for each sound \n",
    "chrome = []\n",
    "\n",
    "for index, row in audio_df.iterrows():\n",
    "    f_name= '/Users/spence/Documents/GitHub/Urban Sounds Classifier/UrbanSound8K/audio/fold' + str(row[\"fold\"]) + '/' + str(row[\"slice_file_name\"])\n",
    "    audio_ts, sr = lr.load(f_name, res_type='kaiser_fast')\n",
    "    s = np.abs(lr.stft(audio_ts))\n",
    "    c = np.mean(lr.feature.chroma_stft(S=s, sr=sr).T, axis=0)\n",
    "    chrome.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(8724, 12)"
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe dimensions of our newly create 'chrome' list \n",
    "\n",
    "len(chrome) , len(chrome[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(8724, 174)"
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add zeros, as padding to match the chroma array length with mfcc\n",
    "\n",
    "needed_pads = len(mfccs[0][0]) - len(chrome[0])\n",
    "\n",
    "pads = [0 for i in range(0,needed_pads)]\n",
    "\n",
    "chroma = []\n",
    "for l in chrome:\n",
    "    l = np.append(l, pads)\n",
    "    chroma.append(l)\n",
    "\n",
    "\n",
    "# observe new length \n",
    "len(chroma), len(chroma[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append chroma to df \n",
    "\n",
    "audio_df['chroma'] = chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape chroma column to make it compatible with mfccs column; taking each value from a 1-D to a 2-D array \n",
    "\n",
    "audio_df['chroma'] = audio_df['chroma'].apply(lambda x: np.reshape(x, (1, 174)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the 'mfccs' and 'chroma' columns into one column \n",
    "\n",
    "audio_df['features'] = audio_df.apply( lambda row: np.concatenate((row['mfccs'], row['chroma']), axis=0) , axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Zero Crossing Rate - This is the meaure of times that the amplitude of a signal crosses through the zero value in a given time interval. It is very useful in indentifying percussive sounds because it's sensitive to transients, and the ADSR characteristics of sound; that is quick sudden strikes vs. slow rising sound or sound that slowly tails off vs. sound that ends abruptly. I figured it could be useful here because there are diverse characteristics of sound with regard to ADSR in the data set.\n",
    "\n",
    "[More on Zero Crossing Rate](https://www.sciencedirect.com/topics/engineering/zero-crossing-rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the zero crossing rate for each sound \n",
    "\n",
    "zero_cr = []\n",
    "\n",
    "for index, row in audio_df.iterrows():\n",
    "    f_name= '/Users/spence/Documents/GitHub/Urban Sounds Classifier/UrbanSound8K/audio/fold' + str(row[\"fold\"]) + '/' + str(row[\"slice_file_name\"])\n",
    "    audio_ts, sr = lr.load(f_name, res_type='kaiser_fast')\n",
    "    z = lr.feature.zero_crossing_rate(y=audio_ts)\n",
    "    zero_cr.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(8724, 1, 14)"
     },
     "execution_count": 746,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check array lengths \n",
    "\n",
    "len(zero_cr), len(zero_cr[0]), len(zero_cr[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add zeros, as padding to match the array length with mfcc\n",
    "\n",
    "zero = []\n",
    "count = 0\n",
    "\n",
    "for l in zero_cr:\n",
    "    needed_pads = len(mfccs[0][0]) - len(zero_cr[count][0])\n",
    "    pads = [0 for i in range(0,needed_pads)]\n",
    "    m = np.append(l, pads)\n",
    "    zero.append(m)\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(8724, 174)"
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe new length \n",
    "\n",
    "len(zero), len(zero[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column in df\n",
    "\n",
    "audio_df['zero_cr'] = zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape column so it's compatible with 'features' column \n",
    "\n",
    "audio_df['zero_cr'] = audio_df['zero_cr'].apply(lambda x: np.reshape(x, (1, 174)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join our 'zero_cr' feature with the others into single column \n",
    "\n",
    "audio_df['features'] = audio_df.apply( lambda row: np.concatenate((row['features'], row['zero_cr']), axis=0) , axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df \n",
    "\n",
    "audio_df.to_pickle('audio_df_xtra.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_df.drop(['chrome'], axis=1, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python388jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}